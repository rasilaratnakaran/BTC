# -*- coding: utf-8 -*-
"""BTCpred.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/169rKLiZH2ZhtUbuhVIIPb7HFk3kWBJAT
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import VotingRegressor

# Load the CSV data into a Pandas dataframe
data = pd.read_csv('/content/BTC-USD.csv')

# Inspect the data
print(data.head())
print(data.tail())
print(data.info())
print(data.describe())

# Handle missing values
data.dropna(inplace=True)

# Remove duplicate rows
data.drop_duplicates(inplace=True)

# Check for any remaining missing values
print(data.isnull().sum())

# Visualize the data
plt.figure(figsize=(12, 6))
plt.plot(data['Date'], data['Close'], label='Bitcoin Close Price', color='blue')
plt.xlabel('Date')
plt.ylabel('Price')
plt.title('Bitcoin Price Over Time')
plt.legend()
plt.show()

# Calculate daily price changes
data['Price Change'] = data['Close'].diff()

# Calculate moving averages
data['MA_7'] = data['Close'].rolling(window=7).mean()
data['MA_30'] = data['Close'].rolling(window=30).mean()

# Calculate volatility indicators (e.g., Bollinger Bands, Average True Range, etc.)

# You can add more feature engineering here based on your needs

# Scale or normalize features as needed
scaler = StandardScaler()
data[['Close', 'Volume', 'Price Change', 'MA_7', 'MA_30']] = scaler.fit_transform(data[['Close', 'Volume', 'Price Change', 'MA_7', 'MA_30']])

# Convert categorical variables to numeric (if any)
# For example, you can one-hot encode categorical columns using OneHotEncoder

import pandas as pd
from sklearn.model_selection import train_test_split

# Load the original data.csv
data = pd.read_csv('/content/BTC-USD.csv')

# Split the dataset into a training set and a test set (80% training, 20% test)
X = data.drop(['Close', 'Date'], axis=1)
y = data['Close']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create training and test dataframes
train_df = pd.concat([X_train, y_train], axis=1)
test_df = pd.concat([X_test, y_test], axis=1)

# Save the training and test dataframes as train.csv and test.csv
train_df.to_csv('train.csv', index=False)
test_df.to_csv('test.csv', index=False)
data = pd.read_csv('/content/train.csv')

# Drop rows with missing values in both training and test data
X_train.dropna(inplace=True)
y_train = y_train[X_train.index]  # Match the labels with the remaining rows
X_test.dropna(inplace=True)
y_test = y_test[X_test.index]  # Match the labels with the remaining rows

# Fine-tune hyperparameters for the best performing models using GridSearchCV
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
}

grid_rf = GridSearchCV(RandomForestRegressor(), param_grid, cv=5)
grid_rf.fit(X_train, y_train)

# You can do the same for other models

# Try combining multiple models into an ensemble model to improve accuracy (e.g., VotingRegressor)
ensemble_model = VotingRegressor([('lr', model_lr), ('rf', model_rf), ('xgb', model_xgb)])
ensemble_model.fit(X_train, y_train)

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Initialize the models
model_lr = LinearRegression()
model_rf = RandomForestRegressor()
model_xgb = XGBRegressor()

# Fit the models with training data
model_lr.fit(X_train, y_train)
model_rf.fit(X_train, y_train)
model_xgb.fit(X_train, y_train)

# Make predictions for each model
y_pred_lr = model_lr.predict(X_test)
y_pred_rf = model_rf.predict(X_test)
y_pred_xgb = model_xgb.predict(X_test)

# Calculate MAE for each model
mae_lr = mean_absolute_error(y_test, y_pred_lr)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)

# Calculate RMSE for each model
rmse_lr = mean_squared_error(y_test, y_pred_lr, squared=False)
rmse_rf = mean_squared_error(y_test, y_pred_rf, squared=False)
rmse_xgb = mean_squared_error(y_test, y_pred_xgb, squared=False)

# Calculate R-squared for each model
r2_lr = r2_score(y_test, y_pred_lr)
r2_rf = r2_score(y_test, y_pred_rf)
r2_xgb = r2_score(y_test, y_pred_xgb)

# Print the results for each model
print("Linear Regression Metrics:")
print("MAE:", mae_lr)
print("RMSE:", rmse_lr)
print("R-squared:", r2_lr)

print("\nRandom Forest Metrics:")
print("MAE:", mae_rf)
print("RMSE:", rmse_rf)
print("R-squared:", r2_rf)

print("\nXGBoost Metrics:")
print("MAE:", mae_xgb)
print("RMSE:", rmse_xgb)
print("R-squared:", r2_xgb)